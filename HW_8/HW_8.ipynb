{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW_8\n",
    "## Name: XinranGuo 国欣然\n",
    "## StudentID: 2001212337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the PDF files \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = r\"https://openaccess.thecvf.com/ICCV2021?day=2021-10-12\"\n",
    "\n",
    "response = requests.get(url)\n",
    "bs = BeautifulSoup(response.text, \"html.parser\")\n",
    "pdf_tags = bs.find_all(\"dt\", class_=\"ptitle\")\n",
    "taglist=[]\n",
    "for tag in pdf_tags:\n",
    "    taglist.append(tag.a.get(\"href\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import random\n",
    "from hashlib import md5\n",
    "\n",
    "def translate_api(input_text):\n",
    "    # Set your own appid/appkey.\n",
    "    appid = '20211214001028718'\n",
    "    appkey = 'QzIjVNNzjwtQ1ukg3Slu'\n",
    "    def make_md5(s, encoding='utf-8'):\n",
    "        return md5(s.encode(encoding)).hexdigest()\n",
    "    # For list of language codes, please refer to `https://api.fanyi.baidu.com/doc/21`\n",
    "    from_lang = 'en'\n",
    "    to_lang = 'zh'\n",
    "    endpoint = 'http://api.fanyi.baidu.com'\n",
    "    path = '/api/trans/vip/translate'\n",
    "    url = endpoint + path\n",
    "    \n",
    "    query = input_text\n",
    "    # Generate salt and sign\n",
    "    salt = random.randint(32768, 65536)\n",
    "    sign = make_md5(appid + query + str(salt) + appkey)\n",
    "    # Build request\n",
    "    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "    payload = {'appid': appid, 'q': query, 'from': from_lang, 'to': to_lang, 'salt': salt, 'sign': sign}\n",
    "    # Send request\n",
    "    r = requests.post(url, params=payload, headers=headers)\n",
    "    result = r.json()\n",
    "    \n",
    "    return result['trans_result'][0]['dst']\n",
    "\n",
    "print(translate_api('he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the abstract from other url\n",
    "import time\n",
    "\n",
    "s = r'https://openaccess.thecvf.com'\n",
    "n=0\n",
    "for ab_url in taglist:\n",
    "    url = s + ab_url\n",
    "\n",
    "    # response = requests.get(url)\n",
    "    # bs = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # abstract = bs.find_all(\"div\", id=\"abstract\")\n",
    "\n",
    "    response = requests.get(url)\n",
    "    bs = BeautifulSoup(response.text, \"html.parser\")\n",
    "    abstract = bs.find_all(\"div\", id=\"abstract\")\n",
    "    ab_content = abstract[0].get_text()\n",
    "    \n",
    "    n = n+1\n",
    "    #print(ab_content)\n",
    "    trans_result = translate_api(ab_content)\n",
    "    time.sleep(1)\n",
    "    with open(r'D:\\github代码\\2021_PHBS_python_programming_1\\HW_8/result.txt', 'a') as f:\n",
    "        f.write(ab_content + \"\\n\")\n",
    "        f.write(trans_result + \"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB, a large-scale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB to pretrain our Airbert model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "ab_content = abstract[0].get_text()\n",
    "print(ab_content)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1fd8df5d449c77885c36ee13fb077ca8b1254551d08c28de248d875df95e9cb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
